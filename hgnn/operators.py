import torch as th
import torch.nn as nn
import dgl
import torch
from dgl.nn.pytorch.conv import *
import torch.nn.functional as F



def gnn_map(gnn_name, n_inp, n_out, n_heads=4, act=F.relu, dropout=0.6):
    """
        each gnn contain a opt_weight matrix
        each gnn don't contain dropout
    """
    if gnn_name == "gcn_conv":
        # contain opt_weight matrix and normed by nodeâ€™s in-degree
        return GraphConv(n_inp,
                         n_out,
                         norm="right",
                         weight=True,
                         activation=act,
                         allow_zero_in_degree=True)
    elif gnn_name == "gat_conv":
        return MetaGATConv(n_inp,
                           n_out,
                           num_heads=n_heads,
                           feat_drop=dropout,
                           attn_drop=dropout,
                           activation=act,
                           allow_zero_in_degree=True)
    elif gnn_name == "edge_conv":
        return EdgeConv(n_inp,
                        n_out,
                        allow_zero_in_degree=True)
    elif gnn_name == "sage_pool":
        return SAGEConv(n_inp, n_out, aggregator_type="pool", feat_drop=dropout, activation=act)
    elif gnn_name == "zero_conv":
        return HomoZero()
    else:
        raise RuntimeError(f"Wrong gnn type:{gnn_name}")


class HomoZero(torch.nn.Module):
    def __init__(self):
        super(HomoZero, self).__init__()

    def forward(self, G, inputs: tuple):
        return torch.zeros_like(inputs[1]).to(inputs[1].device)


class HeteroSemanticAttention(nn.Module):
    def __init__(self, n_inp, n_hid, node_dict, dropout=0.6):
        super(HeteroSemanticAttention, self).__init__()
        self.atts = nn.ModuleDict()
        for ntype in node_dict:
            self.atts[ntype] = SemanticAttention(n_inp, n_hid, dropout=dropout)

    def forward(self, inputs, dsttype):
        stacked = th.stack(inputs, dim=0)
        return self.atts[dsttype](stacked)


class SemanticAttention(nn.Module):
    def __init__(self,
                 n_inp,
                 n_hid=128,
                 dropout=0.6
                 ):
        super(SemanticAttention, self).__init__()
        self.dropout = nn.Dropout(dropout)
        self.project = nn.Sequential(
            nn.Linear(n_inp, n_hid),
            nn.Tanh(),
            nn.Linear(n_hid, 1, bias=False)
        )

    def forward(self, z):
        """
        :param z: [n_metapath, n_nodes, n_hid ]
        :return:
        """
        z = z.transpose(0, 1)
        z = self.dropout(z)
        w = self.project(z).mean(0)  # (M, 1)
        beta = th.softmax(w, dim=0)  # (M, 1)
        beta = beta.expand((z.shape[0],) + beta.shape)  # (N, M, 1)

        return (beta * z).sum(1)  # (N, n_hid)


def get_aggregate_fn(agg, n_inp=256, n_hid=128, node_dict=None, dropout=0.6):
    """Internal function to get the aggregation function for node data
    generated from different relations.

    Parameters
    ----------
    agg : str
        Method for aggregating node features generated by different relations.
        Allowed values are 'sum', 'max', 'min', 'mean', 'stack'.
    n_inp: int
        the dimension of input vector
    n_inp: int
        the dimension of hidden vector


    Returns
    -------
    callable
        Aggregator function that takes a list of tensors to aggregate
        and returns one aggregated tensor.
    """
    if agg == 'sum':
        fn = th.sum
    elif agg == 'max':
        fn = lambda inputs, dim: th.max(inputs, dim=dim)[0]
    elif agg == 'min':
        fn = lambda inputs, dim: th.min(inputs, dim=dim)[0]
    elif agg == 'mean':
        fn = th.mean
    elif agg in ['att', "stack"]:
        fn = None  # will not be called
    else:
        raise RuntimeError('Invalid cross type aggregator. Must be one of '
                           '"sum", "max", "min", "mean" or "stack". But got "%s"' % agg)
    if agg == 'stack':
        def stack_agg(inputs, dsttype):  # pylint: disable=unused-argument
            if len(inputs) == 0:
                return None
            return th.stack(inputs, dim=1)

        return stack_agg
    elif agg == 'att':
        return HeteroSemanticAttention(n_inp, n_hid, node_dict, dropout=dropout)
    else:
        def aggfn(inputs, dsttype):  # pylint: disable=unused-argument
            if len(inputs) == 0:
                return None
            stacked = th.stack(inputs, dim=0)
            return fn(stacked, dim=0)

        return aggfn


class MetaGATConv(GATConv):
    def forward(self, graph, feat):
        res = super(MetaGATConv, self).forward(graph, feat)
        return res.sum(dim=1)


class MetaDotGATConv(DotGatConv):
    def forward(self, graph, feat):
        res = super(MetaDotGATConv, self).forward(graph, feat)
        return res.sum(dim=1)


import torch.nn as nn
import torch.nn.functional as F
import dgl


class HeteroMLP(nn.Module):
    def __init__(self,
                 n_inp: int,
                 n_hid: int,
                 n_out: int,
                 n_layer: int = 2,
                 act=F.relu,
                 dropout:float = 0.2,
                 residual=False,
                 bias: bool = False):
        assert n_layer > 0
        super(HeteroMLP, self).__init__()
        self.layers = nn.ModuleList()
        self.act = act
        self.dropout = nn.Dropout(dropout)
        for i in range(n_layer):
            in_dim = n_inp if i == 0 else n_hid
            out_dim = n_out if i == n_layer - 1 else n_hid
            self.layers.append(nn.Linear(in_dim, out_dim, bias=bias))

        self.residual = residual
        if residual:
            self.residual_layer = nn.Linear(n_inp, n_out, bias=bias)

    def forward(self, G:dgl.DGLHeteroGraph, inputs: dict):
        if G.is_block:
            h = {k: v[:G.number_of_dst_nodes(k)] for k, v in inputs.items()}
        else:
            h = inputs
        for key in G.ntypes:
            for layer in self.layers:
                h[key] = self.dropout(self.act(layer(h[key])))
        if self.residual:
            for key in G.ntypes:
                h[key] = self.act(self.residual_layer(inputs[key]))
        return h


class HeteroAdaptMLP(nn.Module):
    def __init__(self,
                 n_inp: int,
                 n_hid: int,
                 n_out: int,
                 node_type:(list, dict),
                 n_layer: int = 2,
                 act=F.relu,
                 dropout=0.6,
                 residual=False,
                 bias: bool = False):
        assert n_layer > 0
        super(HeteroAdaptMLP, self).__init__()
        self.layers = nn.ModuleList()
        self.act = act
        self.node_type = node_type
        self.residual = residual
        for i in range(n_layer):
            out_dim = n_out if i == n_layer - 1 else n_hid
            layer = nn.ModuleDict()
            for node_type in self.node_type:
                if i == 0:
                    in_dim = n_inp if isinstance(n_inp, int) else n_inp[node_type]
                else:
                    in_dim = n_hid
                layer[str(node_type)] = nn.Linear(in_dim, out_dim, bias=bias)
            self.layers.append(layer)
        if residual:
            self.residual_layer = nn.ModuleDict()
            for node_type in self.node_type:
                self.residual_layer[node_type] = nn.Linear(n_inp, n_out, bias=bias)

    def forward(self, G:dgl.DGLHeteroGraph, inputs: dict):
        if not isinstance(G, dict) and G.is_block:
            h = {k: v[:G.number_of_dst_nodes(k)] for k, v in inputs.items()}
        else:
            h = inputs
        outputs = [h]
        for layer in self.layers:
            h = {}
            for key in self.node_type:
                h[key] = self.act(layer[key](outputs[-1][key]))
            outputs.append(h)
        h = outputs[-1]
        if self.residual:
            for key in self.node_type:
                h[key] = self.act(self.residual_layer[key](inputs[key]))
        return h


class HeteroAdaptMLPNew(nn.Module):
    def __init__(self,
                 n_inp: int,
                 n_hid: int,
                 n_out: int,
                 node_type:(list, dict),
                 n_layer: int = 2,
                 act=F.relu,
                 dropout: float = 0.2,
                 residual=False,
                 bias: bool = False):
        assert n_layer > 0
        super(HeteroAdaptMLPNew, self).__init__()
        self.layers = nn.ModuleList()
        self.act = act
        self.node_type = node_type
        self.residual = residual
        self.dropout = nn.Dropout(dropout)
        for i in range(n_layer):
            in_dim = n_inp if i == 0 else n_hid
            out_dim = n_out if i == n_layer - 1 else n_hid
            layer = nn.ModuleDict()
            for node_type in self.node_type:
                layer[node_type] = nn.Linear(in_dim, out_dim, bias=bias)
            self.layers.add_module(layer)
        if residual:
            self.residual_layer = nn.ModuleDict()
            for node_type in self.node_type:
                self.residual_layer[node_type] = nn.Linear(n_inp, n_out, bias=bias)

    def forward(self, G:dgl.DGLHeteroGraph, inputs: dict):
        if G.is_block:
            h = {k: v[:G.number_of_dst_nodes(k)] for k, v in inputs.items()}
        else:
            h = inputs
        outputs = [h]
        for layer in self.layers:
            h = {}
            for key in self.node_type:
                h[key] = self.dropout(self.act(layer[key](outputs[-1][key])))
            outputs.append(h)
        h = outputs[-1]
        if self.residual:
            for key in G.ntypes:
                h[key] = self.act(self.residual_layer[key](inputs[key]) + h[key])
        return h
